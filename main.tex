\documentclass[20pt,margin=1in,innermargin=-4.5in,blockverticalspace=-0.25in]{tikzposter}
\geometry{paperwidth=42in,paperheight=30in}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage[backend=biber,style=chicago-authordate]{biblatex}
\usepackage{emory-theme}
\usepackage{caption}
\usepackage{mwe} % for placeholder images

\addbibresource{refs.bib}
\nocite{*}

% set theme parameters
\tikzposterlatexaffectionproofoff
\usetheme{EmoryTheme}
\usecolorstyle{EmoryStyle}

\title{\uppercase{Tree-Based Multiple Imputation Methods}}
\author{Michael Dellermann, Anatol Sluchych, and Jonah Wermter}
\titlegraphic{\includegraphics[scale=1.3]{fu_logo.jpg}}

% begin document
\begin{document}
\maketitle
\centering
\begin{columns}
    \column{0.32}
    \block{1. Motivation}{
        \vspace{-1em}
        \begin{itemize}
            \item Parametric MICE methods: conditional models to be specified for \textit{all} variables with missing data
            \vspace{-0.25em}
            \item Still may fail to capture interactive and nonlinear relations among variables as well as non-standard distributions
            \vspace{-0.25em}
            \item Tree-based methods \textit{automatically} capture interactions, nonlinear relations, and complex distributions with no parametric assumptions or data transformations needed (Burgette \& Reiter 2010)
            \vspace{-0.25em}
            \item Implementation in R: \textit{mice} and \textit{miceRanger} packages
        \end{itemize}
    }
    \block{2. Tree-based methods}{
        \vspace{-1em}
        Classification and regression trees (CART):
        \vspace{-0.5em}
        \begin{itemize}
            \item seek to approximate  conditional distribution of univariate outcome from multiple predictors
            \vspace{-0.25em}
            \item segment predictor space into non-overlapping regions with relatively homogeneous outcomes
            \vspace{-0.25em}
            \item segments found by recursive binary splits of  predictors
            \vspace{-0.25em}
            \item prediction for observations that fall into the same region is mean (or mode) of response values for training observations in region
            \vspace{-0.25em}
            \item may be very non-robust and have lower predictive accuracy 
        \end{itemize}
        
        \vspace{-2em}
        \begin{tikzfigure}[Example of tree structure. Source: Hastie et al. (2009)]
            \includegraphics[scale=2.5]{tree.png}
        \end{tikzfigure}
        \vspace{-1em}
        
        Random forest:
        \vspace{-0.5em}
        \begin{itemize}
            \item \textit{ensemble} method that addresses non-robustness and low predictive accuracy
            \vspace{-0.25em}
            \item average predictions from $B$ non-pruned trees constructed using $B$ bootstrapped training sets 
            \vspace{-0.25em}
            \item \textit{decorrelates} trees by performing each split on \textit{randomly} chosen subset of predictors
        \end{itemize} 
    }

    \block{3. Imputation algorithm}{
    \vspace{-1em}
    4-steps algorithm:
    \vspace{-0.5em}
    \begin{enumerate}
        \item Initial values for the missing values filled in as follows:
        \vspace{-0.25em}
        \begin{enumerate}
            \item Define a matrix $Z$ equal to $Y_c$
            \vspace{-0.25em}
            \item Impute missing values in $Y_i$, where $i=1, ... p_1$, using tree-based method on $Z$ and append the completed version of $Y_i$ to $Z$ prior to incrementing $i$             
        \end{enumerate}
    \end{enumerate}  
    \vspace{-1em}
}

    \column{0.36}
    \block{}{
        \vspace{-1em}
        \begin{enumerate}
            \setcounter{enumi}{1}
            \item  Replace the originally missing values of $Y_i$, where $i=1, ... p_1$, with tree-based methods on $Y_{-i}$
            \vspace{-0.25em}
            \item Repeat $l$ times step 2
            \vspace{-0.25em}
            \item Repeat steps 1–3 $m$ times and obtain $m$ imputed sets.
        \end{enumerate} 
        \vspace{-1em}

  %      \begin{itemize}
  %          \item sequential CART imputation algorithm
  %          \item  order the variables from least amount to largest amount of missing data
  %          \item minimum leaf size of 5 and the splitting criteria of a deviance greater than 0.0001
  %          \item trees are not pruned to minimize bias
  %          \item size of trees modulated by requiring a minimum number of observations in each leaf and by %controlling the minimum heterogeneity in the values in the leaf in order to consider it for further splitting     
  %          \item We take draws from the predictive distribution by sampling elements from the leaf that corresponds to the covariate values of the record of interest
  %          \item  actually perform a Bayesian bootstrap within each leaf before sampling.
   %     \end{itemize}
 }

        \block{4. Comparison mice/miceRanger packages}{
        \vspace{-2em}
        \begin{itemize}
            \item both implement van Buuren’s multivariate imputation by chained equations
            \vspace{-0.25em}
            \item \textit{mice} supports variety of imputation methods, \textit{miceRanger} only random forest
            \vspace{-0.25em}
            \item \textit{mice} uses common R packages \textit{rpart} and \textit{randomForest} to implement tree based imputation methods (van Buuren 2023)
            \vspace{-0.25em}
            \item \textit{miceRanger} uses the \textit{ranger} package instead, which claims to be faster and more efficient with medium and large data sets (Wilson 2022)
                \vspace{-0.25em}
                \begin{itemize}
                    \item[$\Rightarrow$] core functions written in C++ (faster than R, compiled vs. interpreted code) (Wright \& Ziegler 2017)
                    \vspace{-0.25em}
                     \item[$\Rightarrow$] lacks pooling function
                \end{itemize}
        \end{itemize}
        \vspace{-1em}
    }
            \block{5. Empirical simulation study}{ 
            \vspace{-1em}
            Empirical data set:
            \vspace{-0.25em}
            \begin{itemize}
                \item RAND's Health Insurance Experiment:  $n = 20185$, $k = 46$
            \end{itemize}
            \vspace{-0.25em}
            Missing data mechanisms:
            \vspace{-0.25em}
            \begin{itemize}
                \item p=25\% and 50\%
            \end{itemize}
            \vspace{-1em}
            \begin{itemize}
                \item MAR with $\rho=0,\tau=0$: $P(mdvis\_miss \mid xage<25) = p$, \\
                    $P(mdvis\_miss \mid mhi>74) = p$ 
                \vspace{-0.25em}
                \item MCAR: $P(income\_miss) = p$, $P(educdec\_miss) = p$
            \end{itemize}
            \vspace{-0.5em}
            Monte Carlo simulation: R = 1000, M = 5, n = 2000, niter = 10, nrtree = 10 
    }

        \block{6. Results}{
         \vspace{-2em}
        \begin{tikzfigure}[Imputation Time per Subset per Method]
            \includegraphics[width=1\linewidth]{plot_comp_time.png}
        \end{tikzfigure}  
        \vspace{-1em}
    }



    \column{0.32}   

        \block{}{
         \vspace{-2em}
        \begin{tikzfigure}[Boxplot of Coverage Rates by Model]
            \includegraphics[width=1\linewidth]{plot_coverage.png}
        \end{tikzfigure} 
        \vspace{-1em}
    }


    
    \block{7. Conclusion}{
    \vspace{-2em}
    \begin{itemize}
        \item \textit{miceRanger} outperforms other random forest imputation methods, working on average approximately ...\% faster per simulation cycle
        \vspace{-0.25em}
        \item With changing the variability of data types, \textit{miceRanger} works on average ...\% faster per simulation cycle.
        \vspace{-0.25em}
        \item With changing size of data sets, \textit{miceRanger} works on average ...\% fast per simulation cycle
    \end{itemize}
    \vspace{-1em}
    }
    
    \block{References}{
        \vspace{-1em}
        \begin{footnotesize}
        \printbibliography[heading=none]
        \end{footnotesize}
    }
\end{columns}
\end{document}

        \centering
        \begin{tabular}{|l|l|lll|}
             \hline   
             Metric &  Method & Bias \hspace{3mm} & MSE \hspace{3mm} & Coverage \hspace{3mm}\\
             \hline
             mean(age) & BD & NA & NA & NA\\
             mean(age) & CC & NA & NA & NA\\
             mean(age) & CART & NA & NA & NA\\
             mean(age) & RandomForest & NA & NA & NA\\
             mean(age) & miceRanger  & NA & NA & NA \\
             \hline
             mean(educ) & BD & NA & NA & NA\\
             mean(educ) & CC & NA & NA & NA\\
             mean(educ) & CART & NA & NA & NA\\
             mean(educ) & RandomForest & NA & NA & NA\\
             mean(educ) & miceRanger  & NA & NA & NA \\
             \hline
             $\rho$(mdvis, hltg)  & BD & NA & NA & NA\\
             $\rho$(mdvis, hltg) & CC & NA & NA & NA\\
             $\rho$(mdvis, hltg) & CART & NA & NA & NA\\
             $\rho$(mdvis, hltg) & RandomForest & NA & NA & NA\\
             $\rho$(mdvis, hltg) & miceRanger  & NA & NA & NA \\
             \hline
             reg.(mhi)  & BD & NA & NA & NA\\
             reg.(mhi) & CC & NA & NA & NA\\
             reg.(mhi) & CART & NA & NA & NA\\
             reg.(mhi) & RandomForest & NA & NA & NA\\
             reg.(mhi) & miceRanger  & NA & NA & NA \\
             \hline
        \end{tabular}
        \captionof{table}{Simulation results}
}



    
        Following Burgette \& Reiter (2009), let $Y$ be $n \times p$ the data matrix arranged as $Y= (Y_p, Y_c)$, where
        \begin{itemize}
            \item $Y_p$ consists of $p_1$ \textit{partially observed} columns, such that moving from left to right, the number of missing elements in each column is nondecreasing
            \item  $Y_c$ remaining completely observed columns
            \item  $Y_{obs}$ set of observed and $Y_{mis}$ set of missing elements 
        \end{itemize}