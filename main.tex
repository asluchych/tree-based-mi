\documentclass[20pt,margin=1in,innermargin=-4.5in,blockverticalspace=-0.25in]{tikzposter}
\geometry{paperwidth=42in,paperheight=30in}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage[backend=biber,style=chicago-authordate]{biblatex}
\usepackage{emory-theme}

\usepackage{mwe} % for placeholder images

\addbibresource{refs.bib}
\nocite{*}

% set theme parameters
\tikzposterlatexaffectionproofoff
\usetheme{EmoryTheme}
\usecolorstyle{EmoryStyle}

\title{\uppercase{Tree-Based Multiple Imputation Methods}}
\author{Michael Dellermann, Anatol Sluchych, and Jonah Wermter}
\titlegraphic{\includegraphics[scale=1.3]{fu_logo.jpg}}

% begin document
\begin{document}
\maketitle
\centering
\begin{columns}
    \column{0.32}
    \block{1. Motivation}{
        \begin{itemize}
            \item Standard MICE approach: conditional models to be specified for \textit{all} variables with missing data
            \item Still may fail to capture interactive and nonlinear relations among variables as well as non-standard distributions
            \item Classiﬁcation and regression trees (CART) \textit{automatically} capture interactions, nonlinear relations, and complex distributions with no parametric assumptions or data transformations needed (Burgette \& Reiter 2010)
            \item Implementation in R: \textit{mice} and \textit{miceranger} packages
        \end{itemize}
    }
    \block{2. Tree-based methods}{
        
        Description of MICE approach?
        Detailed description of trees?
        
        \vspace{10mm}
        
        CART:
        \begin{itemize}
            \item seek to approximate the conditional distribution of a univariate outcome from multiple predictors
            \item  partition the predictor space so that subsets of units formed by the partitions have relatively homogeneous outcomes
            \item partitions are found by recursive binary splits of the predictors
            \item  series of splits can be effectively represented by a tree structure, with leaves corresponding to the subsets of units
            \item values in each leaf represent the conditional distribution of the outcome for units in the data with predictors that satisfy the partitioning criteria that deﬁne the leaf
        \end{itemize}

        \vspace{1em}
        \begin{tikzfigure}[Example of a tree structure. Source: Burgette \& Reiter (2010)]
            \includegraphics[width=0.6\linewidth]{tree.png}
        \end{tikzfigure}
        \vspace{1em}

        Disadvantages relative to parametric models:
        \begin{itemize}
            \item decreased efﬁciency when the parametric models are adequate
            \item discontinuities at partition boundaries
            \item categorical predictors with many levels can cause computational difﬁculties
        \end{itemize} 
    }

    \column{0.36}
    \block{3. Imputation algorithm}{
    
        Let $Y$ be $n \times p$ the data matrix arranged as $Y= (Y_p, Y_c)$, where
        \begin{itemize}
            \item $Y_p$ consists of $p_1$ \textit{partially observed} columns, such that moving from left to right, the number of missing elements in each column is nondecreasing
            \item  $Y_c$ remaining completely observed columns
            \item  $Y_{obs}$ set of observed and $Y_{mis}$ set of missing elements 
        \end{itemize}

        \vspace{10mm}



        4-steps algorithm:
        \begin{enumerate}
            \item Initial values for the missing values filled in as follows:
            \begin{enumerate}
                \item Define a matrix $Z$ equal to $Y_c$
                \item Impute missing values in $Y_i$, where $i=1, ... p_1$, using CART on $Z$ and append the completed version of $Y_i$ to $Z$ prior to incrementing $i$             
            \end{enumerate}       
            \item  Replace the originally missing values of $Y_i$, where $i=1, ... p_1$, with CART on $Y_{-i}$
            \item Repeat $l$ times step 2
            \item Repeat steps 1–3 $m$ times and obtain $m$ imputed sets.
        \end{enumerate}   


        \vspace{10mm}

        \begin{itemize}
            \item sequential CART imputation algorithm
            \item  order the variables from least amount to largest amount of missing data
            \item minimum leaf size of 5 and the splitting criteria of a deviance greater than 0.0001
            \item trees are not pruned to minimize bias
            \item size of trees modulated by requiring a minimum number of observations in each leaf and by controlling the minimum heterogeneity in the values in the leaf in order to consider it for further splitting     
            \item We take draws from the predictive distribution by sampling elements from the leaf that corresponds to the covariate values of the record of interest
            \item  actually perform a Bayesian bootstrap within each leaf before sampling.
        \end{itemize}
    }

    
    \block{4. Simulation study}{
        \begin{itemize}
            \item interactions among the variables in these domains, rather than
         main effects alone, are likely to be predictors
            \item nature of these interactions is not known a prior
            \item imputations of missing data must
be ﬂexible enough to capture the most important interac-
tions in the data
            \item check the plausibility of our
imputation models using posterior predictive checks
        \end{itemize}

    }

    \column{0.32}
    \block{5. Results}{

    }
    
    \block{6. Conclusion}{
        \begin{itemize}
            \item  MICE by CART imputation  can result in more reliable inferences compared
with naive applications of MICE based on main-effects generalized linear models
            \item For the quadratic and interaction terms, CART-based
MICE results in notably lower mean-squared errors and biases
        \end{itemize}

    }
    
    \block{7. Next steps}{
        \begin{itemize}
            \item random forests
            \item neural networks
            \item Bayesian additive regression trees
        \end{itemize}
    }
    
    \block{References}{
        \vspace{-1em}
        \begin{footnotesize}
        \printbibliography[heading=none]
        \end{footnotesize}
    }
\end{columns}
\end{document}