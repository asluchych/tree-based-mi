\documentclass[20pt,margin=1in,innermargin=-4.5in,blockverticalspace=-0.25in]{tikzposter}
\geometry{paperwidth=42in,paperheight=30in}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage[backend=biber,style=apa6]{biblatex}
\usepackage{emory-theme}
\usepackage{caption}
\usepackage{mwe} % for placeholder images


\addbibresource{refs.bib}
\nocite{*}

% set theme parameters
\tikzposterlatexaffectionproofoff
\usetheme{EmoryTheme}
\usecolorstyle{EmoryStyle}

\title{\uppercase{Tree-Based Multiple Imputation Methods}}
\author{Michael Dellermann, Anatol Sluchych, and Jonah Wermter}
\titlegraphic{\includegraphics[scale=1.3]{fu_logo.jpg}}

% begin document
\begin{document}
\maketitle
\centering
\begin{columns}
    \column{0.32}
    \block{1. Motivation}{
        \vspace{-1em}
        \begin{itemize}
            \item Parametric MICE methods: conditional models to be specified for \textit{all} variables with missing data (van Buuren \& Groothuis-Oudshoorn, 2011)
            \vspace{-0.5em}
            \item Still may fail to capture interactive and nonlinear relations among variables as well as non-standard distributions
            \vspace{-0.5em}
            \item Tree-based methods \textit{automatically} capture interactions, nonlinear relations, and complex distributions with no parametric assumptions or data transformations needed (Burgette \& Reiter, 2010)
            \vspace{-0.5em}
            \item Implementation in R: \textit{mice}, \textit{miceRanger}, and \textit{missRanger} packages
        \end{itemize}
    }
    \block{2. Tree-based methods}{
        \vspace{-1em}
        Classification and regression trees (CART):
        \vspace{-0.5em}
        \begin{itemize}
            \item seek to approximate  conditional distribution of univariate outcome from multiple predictors
            \vspace{-0.5em}
            \item segment predictor space into non-overlapping regions with relatively homogeneous outcomes
            \vspace{-0.5em}
            \item segments found by recursive binary splits of  predictors
            \vspace{-0.5em}
            \item prediction for observations that fall into the same region is mean (or mode) of response values for training observations in region
            \vspace{-0.5em}
            \item may be very non-robust and have relatively low predictive accuracy 
        \end{itemize}
        
        \vspace{-1.5em}

        \begin{center}
        \captionof{figure}{Example of tree structure. Source: Hastie, Tibshirani, \& Friedman (2009)}
        \begin{tikzfigure}[]
            \includegraphics[scale=1.9]{tree.png}
        \end{tikzfigure}
        \end{center}

        \vspace{-3em}
        
        Random forest:
        \vspace{-0.5em}
        \begin{itemize}
            \item \textit{ensemble} method that addresses non-robustness and low predictive accuracy
            \vspace{-0.5em}
            \item average predictions from $B$ non-pruned trees constructed using $B$ bootstrapped training sets 
            \vspace{-0.5em}
            \item \textit{decorrelates} trees by performing each split on \textit{randomly} chosen subset of predictors
            \vspace{-0.5em}
            \item accurate model to impute missing values (Stekhoven \& Bühlmann, 2011)
            \vspace{-1em}
        \end{itemize} 
    }

    \block{3. Imputation algorithm}{
    \vspace{-1em}

  %      \begin{itemize}
  %          \item sequential CART imputation algorithm
  %          \item  order the variables from least amount to largest amount of missing data
  %          \item minimum leaf size of 5 and the splitting criteria of a deviance greater than 0.0001
  %          \item trees are not pruned to minimize bias
  %          \item size of trees modulated by requiring a minimum number of observations in each leaf and by %controlling the minimum heterogeneity in the values in the leaf in order to consider it for further splitting     
  %          \item We take draws from the predictive distribution by sampling elements from the leaf that corresponds to the covariate values of the record of interest
  %          \item  actually perform a Bayesian bootstrap within each leaf before sampling.
  %     \end{itemize}

    
    4-steps algorithm:
    \vspace{-0.5em}
    \begin{enumerate}
        \item Initial values for the missing values filled in as follows:
        \vspace{-0.5em}
        \begin{enumerate}
            \item Define a matrix $Z$ equal to $Y_c$ (ordered matrix according to missingness)
            \vspace{-0.25em}
            \item Impute missing values in $Y_i$, $i=1, ... p_1$, using tree-based method on $Z$ and append the completed version of $Y_i$ to $Z$ prior to incrementing $i$             
        \end{enumerate}
    \vspace{-1em}
    \item  Replace the originally missing values of $Y_i$, $i=1, ... p_1$, with tree-based methods on $Y_{-i}$
    \vspace{-0.5em}
    \item Repeat step 2 $l$  times ($l$  iterations)
    \vspace{-0.5em}          
    \item Repeat steps 1–3 $m$ times and obtain $m$ imputed sets
    \vspace{-0.5em}
    \item Pool $m$ datasets to one completed according to Rubin’s rules  
    \end{enumerate}  
    \vspace{-1em}
}

    \column{0.36}
        \block{4. Comparison \textit{mice}, \textit{miceRanger} \& \textit{missRanger}}{
        \vspace{-2em}
        \begin{itemize}
            \item Packages \textit{mice} and \textit{miceRanger} implement van Buuren’s multivariate imputation by chained equations, \textit{missRanger} by default single imputations (based on \textit{missForest})
            \vspace{-0.5em}
            \item \textit{mice} supports variety of imputation methods, \textit{miceRanger} \& \textit{missRanger} only random forest
            \vspace{-0.5em}
            \item All by default use \textit{ranger} package for random forests (van Buuren, 2023; Mayer, 2023; Wilson, 2022), which claims to be faster and more efficient with larger data sets and complex settings than common R packages (Wilson, 2022)
                \vspace{-0.5em}
                \begin{itemize}
                    \item[$\Rightarrow$] core functions written in C++ (faster than R, compiled vs. interpreted code) (Wright \& Ziegler, 2017)
                    \vspace{-0.5em}
                \end{itemize}
            \item main differences in default values and variety of analytical functions
        \end{itemize}
        \vspace{-1em}
    }
    
            \block{5. Empirical simulation study}{ 
            \vspace{-1em}
            Empirical data set:
            \vspace{-0.5em}
            \begin{itemize}
                \item RAND's Health Insurance Experiment:  $n = 20185$, $k = 46$
            \end{itemize}
            \vspace{-0.5em}
            Missing data mechanisms:
            \vspace{-0.5em}
            \begin{itemize}
                \item p=25\% and 50\%
            \end{itemize}
            \vspace{-1.5em}
            \begin{itemize}
                \item MAR with $\rho=0,\tau=0$: $P(mdvis\_miss \mid xage<25) = p$, \\
                    $P(mdvis\_miss \mid mhi>74) = p$ 
                \vspace{-0.5em}
                \item MCAR: $P(income\_miss) = p$, $P(educdec\_miss) = p$
            \end{itemize}
            \vspace{-0.5em}
            Monte Carlo simulation: $R$ = 100, $M$ = 5, $n$ = 1000, $niter$ = 10, $nrtree$ = 10
            \vspace{-0.5em}
            \begin{itemize}
                \item six subsets: three focus on data types, three on dataset size
            \end{itemize}
            \vspace{-1em}
    }


        \block{6. Results}{
        
        \vspace{-1em}

        \begin{center}
        \captionof{table}{Simulation results}
        \vspace{-0.5em}
        \begin{tabular}{|l|l|lll|}
             \hline   
             Metric &  Method & Bias \hspace{10mm} & MSE \hspace{5mm} & Coverage\\
             \hline
             mean(income) & BD & 6.66  & 15,728 & 0.98\\
             mean(income) & CART & 7.87 & 17,240 & 0.98\\
             mean(income) & RandomForest & 8.19 & 20,813 & 0.95\\
             mean(income) & miceRanger & 17.45 & 18,957 & 0.97\\
             mean(income) & missRanger  & 3.12 & 17,711 & 0.95 \\
             \hline
             mean(mdvis|xage>25) & BD & 0.01 & 0.042 & 0.96\\
             mean(mdvis|xage>25) & CART & 0.01 & 0.042 & 1\\
             mean(mdvis|xage>25) & RandomForest & 0.01 & 0.042 & 1\\
             mean(mdvis|xage>25) & miceRanger & 0.01 & 0.042 & 0.96\\
             mean(mdvis|xage>25) & missRanger  & 0.01 & 0.042 & 0.96\\
             \hline
             reg. intercept (ghindx) & BD & 0.05 & 4.85 & 0.91\\
             reg. intercept (ghindx) & CART & 0.67 & 6.90 & 0.95\\
             reg. intercept (ghindx) & RandomForest & 1.34 & 6.17 & 0.96\\
             reg. intercept (ghindx) & miceRanger & 1.76 & 10.96 & 0.89\\
             reg. intercept (ghindx) & missRanger  & 3.38 & 21.98 & 0.73\\
             \hline
        \end{tabular}
        \end{center}
    }



    \column{0.32}   

        \block{}{
        \vspace{-2.5em}
        
        \begin{center}
        \captionof{figure}{Imputation time per subset per method}
        \vspace{-1em}
        \begin{tikzfigure}[]
            \includegraphics[width=1\linewidth]{plot_comp_time.png}
        \end{tikzfigure}
        \end{center}
       
        \vspace{-2em}

        \begin{center}
        \captionof{figure}{Coverage rate by model}
        \vspace{-0.5em}
        \begin{tikzfigure}[]
            \includegraphics[width=1\linewidth]{plot_coverage.png}
        \end{tikzfigure}
        \end{center}
        
        \vspace{-1em}

    }
  
    \block{7. Conclusion}{
    \vspace{-2em}
    \begin{itemize}
        \item Speed: \textit{miceRanger} and \textit{missRanger} are significantly faster: about 11 times quicker than \textit{mice}-CART and X times than \textit{mice}-random forest
        \vspace{-0.5em}
        \item Accuracy \& efficiency: \textit{mice} demonstrates superior accuracy and efficiency in imputing missing data
        \vspace{-0.5em}
        \item Robustness: \textit{mice} exhibits robust performance against various levels of missing data, others less so
        \vspace{-0.5em}
        \item User-friendliness: \textit{mice} offers in depth analytical functions that streamline the analysis process after multiple imputation. \textit{miceRanger} and \textit{missRanger} lack this feature, focusing purely on completing one dataset
        \vspace{-0.5em}
        \item Practical recommendation: For applications where time and computational resources are of the essence, \textit{miceRanger} is recommended. For research, \textit{mice} is preferred for its robustness and built-in analysis features
    \end{itemize}
    \vspace{-1em}
    }
    
    \block{References}{
        \vspace{-2em}
        \begin{footnotesize}
        \printbibliography[heading=none]
        \end{footnotesize}
    }
\end{columns}
\end{document}

        \centering
        \begin{tabular}{|l|l|lll|}
             \hline   
             Metric &  Method & Bias \hspace{3mm} & MSE \hspace{3mm} & Coverage \hspace{3mm}\\
             \hline
             mean(age) & BD & NA & NA & NA\\
             mean(age) & CC & NA & NA & NA\\
             mean(age) & CART & NA & NA & NA\\
             mean(age) & RandomForest & NA & NA & NA\\
             mean(age) & miceRanger  & NA & NA & NA \\
             \hline
             mean(educ) & BD & NA & NA & NA\\
             mean(educ) & CC & NA & NA & NA\\
             mean(educ) & CART & NA & NA & NA\\
             mean(educ) & RandomForest & NA & NA & NA\\
             mean(educ) & miceRanger  & NA & NA & NA \\
             \hline
             $\rho$(mdvis, hltg)  & BD & NA & NA & NA\\
             $\rho$(mdvis, hltg) & CC & NA & NA & NA\\
             $\rho$(mdvis, hltg) & CART & NA & NA & NA\\
             $\rho$(mdvis, hltg) & RandomForest & NA & NA & NA\\
             $\rho$(mdvis, hltg) & miceRanger  & NA & NA & NA \\
             \hline
             reg.(mhi)  & BD & NA & NA & NA\\
             reg.(mhi) & CC & NA & NA & NA\\
             reg.(mhi) & CART & NA & NA & NA\\
             reg.(mhi) & RandomForest & NA & NA & NA\\
             reg.(mhi) & miceRanger  & NA & NA & NA \\
             \hline
        \end{tabular}
        \captionof{table}{Simulation results}
}



    
        Following Burgette \& Reiter (2009), let $Y$ be $n \times p$ the data matrix arranged as $Y= (Y_p, Y_c)$, where
        \begin{itemize}
            \item $Y_p$ consists of $p_1$ \textit{partially observed} columns, such that moving from left to right, the number of missing elements in each column is nondecreasing
            \item  $Y_c$ remaining completely observed columns
            \item  $Y_{obs}$ set of observed and $Y_{mis}$ set of missing elements 
        \end{itemize}