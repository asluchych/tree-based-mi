\documentclass[20pt,margin=1in,innermargin=-4.5in,blockverticalspace=-0.25in]{tikzposter}
\geometry{paperwidth=42in,paperheight=30in}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage[backend=biber,style=chicago-authordate]{biblatex}
\usepackage{emory-theme}
\usepackage{caption}
\usepackage{mwe} % for placeholder images

\addbibresource{refs.bib}
\nocite{*}

% set theme parameters
\tikzposterlatexaffectionproofoff
\usetheme{EmoryTheme}
\usecolorstyle{EmoryStyle}

\title{\uppercase{Tree-Based Multiple Imputation Methods}}
\author{Michael Dellermann, Anatol Sluchych, and Jonah Wermter}
\titlegraphic{\includegraphics[scale=1.3]{fu_logo.jpg}}

% begin document
\begin{document}
\maketitle
\centering
\begin{columns}
    \column{0.32}
    \block{1. Motivation}{
        \begin{itemize}
            \item Standard MICE approach: conditional models to be specified for \textit{all} variables with missing data
            \item Still may fail to capture interactive and nonlinear relations among variables as well as non-standard distributions
            \item Tree-based methods \textit{automatically} capture interactions, nonlinear relations, and complex distributions with no parametric assumptions or data transformations needed (Burgette \& Reiter 2010)
            \item Implementation in R: \textit{mice} and \textit{miceRanger} packages
        \end{itemize}
    }
    \block{2. Tree-based methods}{
        Classification and regression trees (CART):
        \begin{itemize}
            \item seek to approximate  conditional distribution of univariate outcome from multiple predictors
            \item segment predictor space into non-overlapping regions with relatively homogeneous outcomes
            \item segments found by recursive binary splits of  predictors
            \item prediction for observations that fall into the same region is mean (or mode) of response values for training observations in region
            \item may be very non-robust and have lower predictive accuracy 
        \end{itemize}
        \begin{tikzfigure}[Example of tree structure. Source: Hastie et al. (2009)]
            \includegraphics[width=0.6\linewidth]{tree.png}
        \end{tikzfigure}
        Random forest:
        \begin{itemize}
            \item \textit{ensemble} method that addresses non-robustness and low predictive accuracy
            \item average predictions from $B$ non-pruned trees constructed using $B$ bootstrapped training sets  
            \item \textit{decorrelates} trees by performing each split on \textit{randomly} chosen subset of predictors
        \end{itemize} 
    }

    \column{0.36}
    \block{3. Imputation algorithm}{
    
        Following Burgette \& Reiter (2009), let $Y$ be $n \times p$ the data matrix arranged as $Y= (Y_p, Y_c)$, where
        \begin{itemize}
            \item $Y_p$ consists of $p_1$ \textit{partially observed} columns, such that moving from left to right, the number of missing elements in each column is nondecreasing
            \item  $Y_c$ remaining completely observed columns
            \item  $Y_{obs}$ set of observed and $Y_{mis}$ set of missing elements 
        \end{itemize}

        \vspace{10mm}



        4-steps algorithm:
        \begin{enumerate}
            \item Initial values for the missing values filled in as follows:
            \begin{enumerate}
                \item Define a matrix $Z$ equal to $Y_c$
                \item Impute missing values in $Y_i$, where $i=1, ... p_1$, using tree-based method on $Z$ and append the completed version of $Y_i$ to $Z$ prior to incrementing $i$             
            \end{enumerate}       
            \item  Replace the originally missing values of $Y_i$, where $i=1, ... p_1$, with tree-based methods on $Y_{-i}$
            \item Repeat $l$ times step 2
            \item Repeat steps 1–3 $m$ times and obtain $m$ imputed sets.
        \end{enumerate}   


        \vspace{10mm}

  %      \begin{itemize}
  %          \item sequential CART imputation algorithm
  %          \item  order the variables from least amount to largest amount of missing data
  %          \item minimum leaf size of 5 and the splitting criteria of a deviance greater than 0.0001
  %          \item trees are not pruned to minimize bias
  %          \item size of trees modulated by requiring a minimum number of observations in each leaf and by %controlling the minimum heterogeneity in the values in the leaf in order to consider it for further splitting     
  %          \item We take draws from the predictive distribution by sampling elements from the leaf that corresponds to the covariate values of the record of interest
  %          \item  actually perform a Bayesian bootstrap within each leaf before sampling.
   %     \end{itemize}
 }

            \block{4. Comparison mice/miceRanger packages}{
        \begin{itemize}
            \item both implement Stef van Buuren’s Multivariate Imputation by Chained Equations
            \item \textit{mice} supports variety of imputation methods, \textit{miceRanger} only randomForest
            \item \textit{mice} uses common R packages “rpart” and “randomForest” to implement tree based imputation methods (van Buuren 2023)
            \item \textit{miceRanger} uses the “ranger”-package instead, which claims to be faster and more efficient with medium and large data sets (Wilson 2022)
                \begin{itemize}
                    \item[$\Rightarrow$] core functions written in C++ (faster than R, compiled vs. interpreted code) (Wright und Ziegler 2017)
                    \item[$\Rightarrow$] supports parallel computing (Wright und Ziegler 2017; Wright 2023)
                \end{itemize}
        \end{itemize}
    }
            \block{5. Empirical simulation study}{
            Objective:
            \begin{itemize}
            \item evaluate efficacy of tree-based imputation methods on missing data
            \item compare \textit{mice} package methods against the extended methods in \textit{miceRanger}
            \end{itemize} 
            
            Empirical data set:
            \begin{itemize}
            \item RAND's Health Insurance Experiment
            \end{itemize}

            Monte Carlo simulation:
            \begin{itemize}
                \item simulations (R): 1000 cycles to ensure robustness
                \item multiple imputations (M): 5 imputations to estimate variability
                \item sample size (n): 2000 cases to ensure generalizability.
  
            \end{itemize}
    }

    \column{0.32}
    \block{}{
    \begin{itemize}
        \item iterations (niter): 10 iterations for chained equations.
        \item random forest trees (nrtree): 10 trees in each RandomForest model for depth
    \end{itemize}
    Comparison Metrics: determine the most accurate and efficient imputation method that best reconstructs true values while minimizing systematic errors
    \begin{itemize}
        \item bias: deviation of imputed values from true values.
        \item mean squared error: average squared difference between the imputed and true values.
        \item coverage: proportion of times true values fall within the calculated confidence intervals.
    \end{itemize}
    
    }
   
    \block{6. Results}{

    

        \centering
        \begin{tabular}{|l|l|lll|}
             \hline   
             Metric &  Method & Bias \hspace{3mm} & MSE \hspace{3mm} & Coverage \hspace{3mm}\\
             \hline
             mean(age) & BD & NA & NA & NA\\
             mean(age) & CC & NA & NA & NA\\
             mean(age) & CART & NA & NA & NA\\
             mean(age) & RandomForest & NA & NA & NA\\
             mean(age) & miceRanger  & NA & NA & NA \\
             \hline
             mean(educ) & BD & NA & NA & NA\\
             mean(educ) & CC & NA & NA & NA\\
             mean(educ) & CART & NA & NA & NA\\
             mean(educ) & RandomForest & NA & NA & NA\\
             mean(educ) & miceRanger  & NA & NA & NA \\
             \hline
             $\rho$(mdvis, hltg)  & BD & NA & NA & NA\\
             $\rho$(mdvis, hltg) & CC & NA & NA & NA\\
             $\rho$(mdvis, hltg) & CART & NA & NA & NA\\
             $\rho$(mdvis, hltg) & RandomForest & NA & NA & NA\\
             $\rho$(mdvis, hltg) & miceRanger  & NA & NA & NA \\
             \hline
             reg.(mhi)  & BD & NA & NA & NA\\
             reg.(mhi) & CC & NA & NA & NA\\
             reg.(mhi) & CART & NA & NA & NA\\
             reg.(mhi) & RandomForest & NA & NA & NA\\
             reg.(mhi) & miceRanger  & NA & NA & NA \\
             \hline
        \end{tabular}
        \captionof{table}{Simulation results}
}




    


    }
    
    \block{7. Conclusion}{

    }
    \block{References}{
        \vspace{-1em}
        \begin{footnotesize}
        \printbibliography[heading=none]
        \end{footnotesize}
    }
\end{columns}
\end{document}